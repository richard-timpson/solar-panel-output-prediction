{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Assignment 6\n",
    "DS 2500 - Data Wrangling  \n",
    "Professor Marina Kogan  \n",
    "University of Utah  \n",
    "\n",
    "Author: Richard Timpson"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I would like to do a time series analysis of home energy consumption data. This consumption data is a perfect candidate for time series analysis because the data consists of a simple timestamp and associated energy usage value (typically in kWh). My overall goal for this assignment is to get a better idea of what energy home patterns are like and to see how consistent they are across different homes. This analysis will include exploring the different temporal relationships in the usage data (hours vs days vs months) and associated visualizations that show the trends. If possible I would also like to explore the distribution of the data and consider some descriptive statistics. If I have the time to build a predictive model I would like to. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "I am pulling from a few different data sources that I have access to and that include home usage data at different timestamps for a few different homes. Some of the data I have already gathered from access to a company named SolarEdge and their API. This includes home energy usage for 4 homes. Another data source I will using is homesese.com and can be pulled as a bulk download. I have 2 homes from their data. \n",
    "\n",
    "The SolarEdge data comes in 15 minute timestamps whereas the homesense comes in 1 hour. SolarEdge's unit of energy measurement is Wh( watt hour) whereas homesense's unit is kWh(kilowatt hour). homesense needs some initial cleaning while SolarEdge has a lot of missing data. I'll need to resample SolarEdge from 15 minute to 1 hour data, rename columns in both data frames, and convert kWh to Wh so that the dataframes are essentially identical"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../../data/consumption_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Sense"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_homesense(site_id):\n",
    "    dfs = []\n",
    "\n",
    "    path1 = f'{DATA_DIR}/homesense/{site_id}/jan18-jan19.csv'\n",
    "    path2 = f'{DATA_DIR}/homesense/{site_id}/jan19-jan20.csv'\n",
    "    path3 = f'{DATA_DIR}/homesense/{site_id}/jan20-jan21.csv'\n",
    "\n",
    "    paths = [path1, path2, path3]\n",
    "    for path in paths:\n",
    "        # the first row of the csv file is not needed\n",
    "        df = pd.read_csv(path,skiprows=1)\n",
    "        dfs.append(df)\n",
    "    hs_df = pd.concat(dfs)\n",
    "    return hs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_dfs = {\n",
    "    1: read_homesense(1), \n",
    "    2: read_homesense(2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data shows that we have detailed usage information in hourly timestamps for several different devices. There are multiple values for each timestamp, but the one that we are interested in is the Device ID \"mains\" and name \"Total Usage\". "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  DateTime  Device ID         Name     Device Type  \\\n0      2018-04-03 19:00:00      mains  Total Usage             NaN   \n1      2018-04-03 19:00:00   f03206dc     Device 1  Mystery Device   \n2      2018-04-03 20:00:00      mains  Total Usage             NaN   \n3      2018-04-03 20:00:00   36a3ac21    Microwave       Microwave   \n4      2018-04-03 20:00:00   ac1cccee      Light 1           Light   \n...                    ...        ...          ...             ...   \n21026  2020-04-07 18:00:00      mains  Total Usage             NaN   \n21027  2020-04-07 18:00:00   2593b8f0          Fan             Fan   \n21028  2020-04-07 18:00:00  always_on    Always On        AlwaysOn   \n21029  2020-04-07 18:00:00   d9ca7cf7      Light 4           Light   \n21030  2020-04-07 18:00:00   03eddf75     Fridge 2          Fridge   \n\n      Device Make Device Model  Device Location  Avg Wattage    kWh  \n0             NaN          NaN              NaN     2509.685  2.510  \n1             NaN          NaN              NaN       62.409  0.062  \n2             NaN          NaN              NaN     1630.022  1.630  \n3             NaN          NaN              NaN       14.746  0.015  \n4             NaN          NaN              NaN       44.216  0.044  \n...           ...          ...              ...          ...    ...  \n21026         NaN          NaN              NaN      544.473  0.544  \n21027         NaN          NaN              NaN        1.263  0.001  \n21028         NaN          NaN              NaN      268.000  0.268  \n21029         NaN          NaN              NaN        2.805  0.003  \n21030         NaN          NaN              NaN      131.524  0.132  \n\n[127804 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime</th>\n      <th>Device ID</th>\n      <th>Name</th>\n      <th>Device Type</th>\n      <th>Device Make</th>\n      <th>Device Model</th>\n      <th>Device Location</th>\n      <th>Avg Wattage</th>\n      <th>kWh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-04-03 19:00:00</td>\n      <td>mains</td>\n      <td>Total Usage</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2509.685</td>\n      <td>2.510</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-04-03 19:00:00</td>\n      <td>f03206dc</td>\n      <td>Device 1</td>\n      <td>Mystery Device</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>62.409</td>\n      <td>0.062</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-04-03 20:00:00</td>\n      <td>mains</td>\n      <td>Total Usage</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1630.022</td>\n      <td>1.630</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-04-03 20:00:00</td>\n      <td>36a3ac21</td>\n      <td>Microwave</td>\n      <td>Microwave</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.746</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-04-03 20:00:00</td>\n      <td>ac1cccee</td>\n      <td>Light 1</td>\n      <td>Light</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>44.216</td>\n      <td>0.044</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21026</th>\n      <td>2020-04-07 18:00:00</td>\n      <td>mains</td>\n      <td>Total Usage</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>544.473</td>\n      <td>0.544</td>\n    </tr>\n    <tr>\n      <th>21027</th>\n      <td>2020-04-07 18:00:00</td>\n      <td>2593b8f0</td>\n      <td>Fan</td>\n      <td>Fan</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.263</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>21028</th>\n      <td>2020-04-07 18:00:00</td>\n      <td>always_on</td>\n      <td>Always On</td>\n      <td>AlwaysOn</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>268.000</td>\n      <td>0.268</td>\n    </tr>\n    <tr>\n      <th>21029</th>\n      <td>2020-04-07 18:00:00</td>\n      <td>d9ca7cf7</td>\n      <td>Light 4</td>\n      <td>Light</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.805</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>21030</th>\n      <td>2020-04-07 18:00:00</td>\n      <td>03eddf75</td>\n      <td>Fridge 2</td>\n      <td>Fridge</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>131.524</td>\n      <td>0.132</td>\n    </tr>\n  </tbody>\n</table>\n<p>127804 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "hs_dfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to filter the data to only have rows with those values. Let's first do a value count to check to make sure that the number of rows with \"mains\" and \"Total Usage\" is the same"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "System 1\nmains        17092\nalways_on    17051\n03eddf75     10730\n000f976b     10658\ne3eaa9b7      7512\nName: Device ID, dtype: int64\nSystem 2\nmains        17608\nalways_on    17577\n6a0788e4     14388\n2cd25dbc     12060\nfe39b6d0     11649\nName: Device ID, dtype: int64\n"
    }
   ],
   "source": [
    "print('System 1')\n",
    "print(hs_dfs[1]['Device ID'].value_counts()[:5])\n",
    "\n",
    "print('System 2')\n",
    "print(hs_dfs[2]['Device ID'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "System 1\nTotal Usage    17092\nAlways On      17051\nFridge 2       10730\nLight 2        10658\nHeat pump       9066\nName: Name, dtype: int64\nSystem 2\nTotal Usage    17608\nAlways On      17577\nFridge 4       14388\nFridge 3       12060\nLight 2        11649\nName: Name, dtype: int64\n"
    }
   ],
   "source": [
    "print('System 1')\n",
    "print(hs_dfs[1]['Name'].value_counts()[:5])\n",
    "\n",
    "print('System 2')\n",
    "print(hs_dfs[2]['Name'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both \"mains\" and \"Total Usage\" have the same count, so we can filter rows that have both of these values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2]:\n",
    "    hs_dfs[i] = hs_dfs[i].loc[(hs_dfs[i]['Device ID'] == \"mains\") & (hs_dfs[i]['Name'] == \"Total Usage\")]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data frame has the correct number of rows"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(17092, 9)\n(17608, 9)\n"
    }
   ],
   "source": [
    "for i in [1,2]:\n",
    "    print(hs_dfs[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now want to clean the data by removing columns that aren't needed, renaming columns, and adjusting the unit to be Wh instead of kWh"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_homesense(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['date'] = pd.to_datetime(df_clean['DateTime'])\n",
    "    df_clean = df_clean.set_index('date')\n",
    "    df_clean = df_clean[['Avg Wattage', 'kWh']]\n",
    "    df_clean['kWh'] = df_clean['kWh'].apply(lambda x: x * 1000)\n",
    "    df_clean = df_clean.rename(columns={'kWh': 'Wh'})\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_dfs_clean = {\n",
    "    1: clean_homesense(hs_dfs[1]),\n",
    "    2: clean_homesense(hs_dfs[2])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     Avg Wattage      Wh\ndate                                    \n2018-04-03 19:00:00     2509.685  2510.0\n2018-04-03 20:00:00     1630.022  1630.0\n2018-04-03 21:00:00     1104.002  1104.0\n2018-04-03 22:00:00      838.236   838.0\n2018-04-03 23:00:00      920.732   921.0\n...                          ...     ...\n2020-04-07 14:00:00      469.358   469.0\n2020-04-07 15:00:00      443.339   443.0\n2020-04-07 16:00:00      499.031   499.0\n2020-04-07 17:00:00      659.337   659.0\n2020-04-07 18:00:00      544.473   544.0\n\n[17092 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Avg Wattage</th>\n      <th>Wh</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-04-03 19:00:00</th>\n      <td>2509.685</td>\n      <td>2510.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 20:00:00</th>\n      <td>1630.022</td>\n      <td>1630.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 21:00:00</th>\n      <td>1104.002</td>\n      <td>1104.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 22:00:00</th>\n      <td>838.236</td>\n      <td>838.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 23:00:00</th>\n      <td>920.732</td>\n      <td>921.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2020-04-07 14:00:00</th>\n      <td>469.358</td>\n      <td>469.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-07 15:00:00</th>\n      <td>443.339</td>\n      <td>443.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-07 16:00:00</th>\n      <td>499.031</td>\n      <td>499.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-07 17:00:00</th>\n      <td>659.337</td>\n      <td>659.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-07 18:00:00</th>\n      <td>544.473</td>\n      <td>544.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17092 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "hs_dfs_clean[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     Avg Wattage      Wh\ndate                                    \n2018-04-03 19:00:00     1149.585  1150.0\n2018-04-03 20:00:00      659.313   659.0\n2018-04-03 21:00:00      821.521   822.0\n2018-04-03 22:00:00     3266.778  3267.0\n2018-04-03 23:00:00     1984.160  1984.0\n...                          ...     ...\n2020-04-08 16:00:00     1246.423  1246.0\n2020-04-08 17:00:00     2000.381  2000.0\n2020-04-08 18:00:00     2979.244  2979.0\n2020-04-08 19:00:00     2007.132  2007.0\n2020-04-08 20:00:00     1618.045  1618.0\n\n[17608 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Avg Wattage</th>\n      <th>Wh</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-04-03 19:00:00</th>\n      <td>1149.585</td>\n      <td>1150.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 20:00:00</th>\n      <td>659.313</td>\n      <td>659.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 21:00:00</th>\n      <td>821.521</td>\n      <td>822.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 22:00:00</th>\n      <td>3266.778</td>\n      <td>3267.0</td>\n    </tr>\n    <tr>\n      <th>2018-04-03 23:00:00</th>\n      <td>1984.160</td>\n      <td>1984.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2020-04-08 16:00:00</th>\n      <td>1246.423</td>\n      <td>1246.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-08 17:00:00</th>\n      <td>2000.381</td>\n      <td>2000.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-08 18:00:00</th>\n      <td>2979.244</td>\n      <td>2979.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-08 19:00:00</th>\n      <td>2007.132</td>\n      <td>2007.0</td>\n    </tr>\n    <tr>\n      <th>2020-04-08 20:00:00</th>\n      <td>1618.045</td>\n      <td>1618.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17608 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "hs_dfs_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\nFalse\n"
    }
   ],
   "source": [
    "print(pd.isnull(hs_dfs_clean[1]['Wh']).values.any())\n",
    "print(pd.isnull(hs_dfs_clean[2]['Wh']).values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no zero rows for both data sets, which is promising"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Edge\n",
    "There are 4 different datasets that come from SolarEdge's api. They consist purely of 15 minute timestamps and associated Wh(watt hour) energy readings. I'll keep all of the SolarEdge data frames in a dictionary. The id's associated with the systems are 466851, 896164, 1520756, 1520780"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [466851, 896164, 1520756, 1520780]\n",
    "se_data = {}\n",
    "for site_id in ids:\n",
    "    df = pd.read_csv(f'{DATA_DIR}/SolarEdge/{site_id}/consumption_data.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # this is key as we are resampling the data to 1 hour timestamps instead of 15 minute. Should \n",
    "    # make for a more consistent analysis across data sets\n",
    "    df = df.resample('1H').sum()\n",
    "    df = df.rename(columns={'value': 'Wh'})\n",
    "    se_data[site_id] = df\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One system has data for 3 years, one roughly 2, and the other only have 2 months worth of data. Some of the systems have a significant amount of null values in the data. I'm not sure how this will affect the analysis but it is important to track this. The best data from the SolarEdge systems is id 896164"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_site_info(site_id):\n",
    "    print(f'Site: {site_id}')\n",
    "    print(f'DF shape: {se_data[site_id].shape}')\n",
    "    print(f'Number of 0 values: {(se_data[site_id][\"Wh\"] == 0).sum()}')\n",
    "    print(f'Number of null values: {pd.isnull(se_data[site_id][\"Wh\"]).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        Wh\ndate                      \n2017-02-24 00:00:00    0.0\n2017-02-24 01:00:00    0.0\n2017-02-24 02:00:00    0.0\n2017-02-24 03:00:00    0.0\n2017-02-24 04:00:00    0.0\n...                    ...\n2020-03-23 19:00:00  731.0\n2020-03-23 20:00:00  812.0\n2020-03-23 21:00:00  748.0\n2020-03-23 22:00:00  890.0\n2020-03-23 23:00:00  748.0\n\n[26976 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Wh</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-02-24 00:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-02-24 01:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-02-24 02:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-02-24 03:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-02-24 04:00:00</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2020-03-23 19:00:00</th>\n      <td>731.0</td>\n    </tr>\n    <tr>\n      <th>2020-03-23 20:00:00</th>\n      <td>812.0</td>\n    </tr>\n    <tr>\n      <th>2020-03-23 21:00:00</th>\n      <td>748.0</td>\n    </tr>\n    <tr>\n      <th>2020-03-23 22:00:00</th>\n      <td>890.0</td>\n    </tr>\n    <tr>\n      <th>2020-03-23 23:00:00</th>\n      <td>748.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>26976 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "se_data[466851]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Site: 466851\nDF shape: (26976, 1)\nNumber of 0 values: 10593\nNumber of null values: 0\nSite: 896164\nDF shape: (12408, 1)\nNumber of 0 values: 9\nNumber of null values: 0\nSite: 1520756\nDF shape: (696, 1)\nNumber of 0 values: 522\nNumber of null values: 0\nSite: 1520780\nDF shape: (696, 1)\nNumber of 0 values: 523\nNumber of null values: 0\n"
    }
   ],
   "source": [
    "for site_id in ids:\n",
    "    print_site_info(site_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "These visualizations are going to be tricky because some homes have a vastly larger number of data points than others. My guess is that the home usage data should be fairly consistent across days with patterns of usage at night and patterns of usage during the day. I would also guess that there are going to be outliers to these patterns that are essentially impossible to predict. Because my overall goal is going to be in building a predictive model I think it is first going to be useful to gain an idea of what the patterns of usage look like at different granularities. I'll list them out.  \n",
    "\n",
    "1. Seasonally: This might include the average monthly usage across the different years. The hypothesis might be that the times of year that are the hottest and coldest will require the most energy because of central heating and cooling. However, some homes use gas rather than electricity for some appliances so it may not stay the same across all of the homes. It would be useful to compare years to each other to see how much it changes per home, if it changes at all \n",
    "2. Daily: What does the home usage look like on a daily basis? Does the usage per day stay consistent or have large variation? Obviously this is going to be affected by the season of the year so there should be a trend that the daily pattern follows (slowly increasing or slowly decreasing). \n",
    "3. Hourly: Is more energy used at night than at day or vise versa? How much variation is there in usage by day? This will obviously be affected by the season of the year as the days get longer/shorter so it may be hard to tell from just this data. If I have time I would like to find an api that will give me the sunrise and sunset times of each day (or find a way to calculate it myself) to see what this trend looks like. \n",
    "\n",
    "The first thing I would like to do is to give a crude visualization of each system so that we can get an idea of what the data looks like. One of the homes had a significant amount of 0 values so it may not even worth be considering in the analysis. The visualization should give us an idea of how to handle that. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crude(df, site_id):\n",
    "    df = df.copy()\n",
    "    f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "    plt.ylabel('Wh')\n",
    "    plt.title(f'System: {site_id}')\n",
    "    df['Wh'].plot(lw=3,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_crude(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_crude(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots are not that representative of the global pattern in the datasets. I'm going to plot the daily average consumption on top of the data which will give a better understanding of the trend. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling(df, site_id):\n",
    "    df = df.copy()\n",
    "    f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "    df['rolling_mean_wh'] = df['Wh'].rolling(24).mean()\n",
    "    plt.ylabel('Wh')\n",
    "    plt.title(f'System: {site_id}')\n",
    "    df['rolling_mean_wh'].plot(ax=ax, lw=3, color='k')\n",
    "    df['Wh'].plot(lw=3,ax=ax, color='k', alpha=.5)\n",
    "    # ax.set_ylim((2e6,6e6))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_rolling(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_rolling(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal\n",
    "Here I would like to view the monthly consumption of each home. I'll do two graphs for this. One will be the monthly consumption for the entire length of the system. The other will be monthly consumption with a comparison by year. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_total(df, site_id):\n",
    "    df = df.copy()\n",
    "    df = df.resample('1M').sum()\n",
    "    f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "    plt.ylabel('Wh')\n",
    "    plt.title(f'System: {site_id}')\n",
    "    df['Wh'].plot(lw=3,ax=ax)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_monthly_total(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_monthly_total(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "def plot_monthly_comparison(df, site_id):\n",
    "    df = df.copy()\n",
    "    df = df.resample('1M').sum()\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    sb.catplot(x='month',y='Wh',hue='year',data=df,kind='point',aspect=2)\n",
    "    plt.title(f'System: {site_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_monthly_comparison(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_monthly_comparison(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are interesting because they show that the usage is not consistent across years. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily\n",
    "I'm going to produce the same graphs as above but on a daily sum instead of monthly sum"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_comparison(df, site_id):\n",
    "    df = df.copy()\n",
    "    # f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "    df = df.resample('1D').sum()\n",
    "    df['day'] = df.index.dayofyear\n",
    "    df['year'] = df.index.year\n",
    "    sb.catplot(x='day',y='Wh',hue='year',data=df,kind='point',height=4, aspect=5)\n",
    "    plt.title(f'System: {site_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_daily_comparison(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_daily_comparison(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the same results from the monthly graph but on a more granular level. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly\n",
    "Here I would like to see the trends for home usage at different hours of the day. One graph wil just be the average production per hour. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_avg(df, site_id):\n",
    "    # f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    groups = df.groupby('hour').mean()\n",
    "    f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "    plt.ylabel('Wh')\n",
    "    plt.title(f'System: {site_id}')\n",
    "    groups['Wh'].plot(lw=3,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_hourly_avg(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_hourly_avg(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am plotting the average hourly production and comparing across months. Different times of the year should have an affect on the hourly production both because of changes in temperature but also somewhat because of the change in length of days. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_comparison(df, site_id):\n",
    "    # f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['month'] = df.index.month\n",
    "    sb.catplot(x='hour',y='Wh',hue='month',data=df,kind='point',height=8, aspect=3, ci=None)\n",
    "    plt.title(f'System: {site_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_hourly_comparison(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_hourly_comparison(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graphs are a little bit dense and difficult to interpret (although they are still useful). A related plot I would like to show is the average hourly production and a comparison across seasons of the year. Winter is season 1, Spring season 2, and so forth"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_comparison_season(df, site_id):\n",
    "    df = df.copy()\n",
    "    # f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "    df['hour'] = df.index.hour\n",
    "    # (temp2.dt.month%12 + 3)//3\n",
    "    df['season'] = (df.index.month%12 + 3) // 3\n",
    "    sb.catplot(x='hour',y='Wh',hue='season',data=df,kind='point',height=4, aspect=5, ci=None)\n",
    "    plt.title(f'System: {site_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    plot_hourly_comparison_season(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    plot_hourly_comparison_season(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these results show me that the variation in usage across the different homes is large. I was interested to see if there would emerge any global pattern among the usage data but so far that has not been the case. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the Data \n",
    "After a visual look at the data I am understanding that usage data is not consistent across homes (at least given the data we have here), but that each home has it's own pattern. I would like to build a predictive model of each home to see how well usage can be predicted just given the temporal pattern.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "To start I would like to perform a simple linear regression model using the different temporal. After viewing the above plots, my guess is that just using the hour of the day and the month of the year should produce a somewhat accurate model. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lin_regression(df, site_id):\n",
    "    print(f\"Regression for site: {site_id}\")\n",
    "    df = df.copy()\n",
    "    df['month'] = df.index.month \n",
    "    df['hour'] = df.index.hour\n",
    "    m2 = smf.ols('Wh ~ C(hour) + C(month)',data=df).fit()\n",
    "    print(m2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting SolarEdge data\")\n",
    "for key,value in se_data.items(): \n",
    "    simple_lin_regression(value, key)\n",
    "\n",
    "print(\"Plotting homesense data\")\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    simple_lin_regression(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the R-squared values are substantially low so this tells me that these variables do not in fact do a good job of explaining the data. Some of the p-values for the t stastitic in the coeeficients are too high to be statistically significant so that is also to be aware of. \n",
    "\n",
    "It's also important to keep in mind that the data for 3 of the systems is extremely flawed. One of the systems has missing data for a year, while 2 of them only have data for a few months. I'm ignoring this here but will take it into account when I build a more sophisticated model. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation\n",
    "Given that the linear regression model with the temporal timescales did not do a good job of explaining the data I'll move to dutocorrelation. The first thing that I would like to do is produce some graphs that show the autocorrelation and the incorporate that into a predictive model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correlation(df, site_id):\n",
    "    df = df.copy()\n",
    "    f,axs = plt.subplots(2,1,figsize=(8,8))\n",
    "    fig1 = sm.graphics.tsa.plot_acf(df['Wh'],zero=False,lags=30,ax=axs[0],alpha=.05)\n",
    "    axs[0].set_title(f'Autocorrelation: System {site_id}')\n",
    "    fig2 = sm.graphics.tsa.plot_pacf(df['Wh'],zero=False,lags=30,ax=axs[1],alpha=.05)\n",
    "    axs[1].set_title(f'Partial Autocorrelation: System {site_id}')\n",
    "\n",
    "    # for ax in axs:\n",
    "    #     ax.axvline(12,c='r',ls='--',lw=1\n",
    "    #     ax.axvline(24,c='r',ls='--',lw=1)\n",
    "    #     ax.axvline(36,c='r',ls='--',lw=1)\n",
    "    #     ax.axvline(48,c='r',ls='--',lw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in se_data.items(): \n",
    "    auto_correlation(value, key)\n",
    "\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    auto_correlation(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show that the first few lags are heavily correlated with each other and at a high level of stastical significance. This is a good sign for a predictive model. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_auto_correlation(df, site_id):\n",
    "    df = df.copy()\n",
    "    f,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "    ax = pd.plotting.autocorrelation_plot(df['Wh'],ax=ax)\n",
    "    plt.title(f'Auto correlation: System {site_id}')\n",
    "    # ax.set_xlim((1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in se_data.items(): \n",
    "    pd_auto_correlation(value, key)\n",
    "\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    pd_auto_correlation(value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs are interesting because they are showing a cycle in the correlation the greater the lag. This indicates that the hour of the day plays a role in the correlation. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Model\n",
    "As was shown in class, we are going to be using the prophet library to build an auto regressive model for the prediction. We'll have to do some column renaming of our data frames to be compatable with the library. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.register_matplotlib_converters()\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_df_columns(df, site_id):\n",
    "    df = df.copy()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"date\": \"ds\", \"Wh\":\"y\" })\n",
    "    df = df[['ds', 'y']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one dictionary to hold all data. Don't know why I didn't do this earlier\n",
    "p_data = {}\n",
    "\n",
    "for key,value in se_data.items(): \n",
    "    df = rename_df_columns(value, key)\n",
    "    p_data[key] = df\n",
    "\n",
    "for key,value in hs_dfs_clean.items(): \n",
    "    df = rename_df_columns(value, key)\n",
    "    p_data[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Plots\n",
    "Prophet makes plotting the data to understand the autoregressive models extremely simple. I'm going to build a forecast model for 30 days and use their plotting functions to show what the trends look like for each system"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prophet(df):\n",
    "    m = Prophet().fit(df)\n",
    "    m_future = m.make_future_dataframe(periods=30, freq='D')\n",
    "    m_forecast = m.predict(m_future)\n",
    "    return m, m_forecast \n",
    "\n",
    "def plot_prophet(m, m_forecast, site_id):\n",
    "    # plot\n",
    "    f,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "    _ = m.plot(m_forecast,ax=ax)\n",
    "\n",
    "    # Always label your axes\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Wh')\n",
    "    ax.set_title(f'Forecasted consumption for system {key}')\n",
    "\n",
    "    m.plot_components(m_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2b2aebca22eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_prophet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplot_prophet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_forecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-23fa40a80365>\u001b[0m in \u001b[0;36mplot_prophet\u001b[0;34m(m, m_forecast, site_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_prophet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_forecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_forecast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = p_data[1]\n",
    "# m, m_forecast = run_prophet(df)\n",
    "# plot_prophet(m, m_forecast, 1)\n",
    "\n",
    "for key, value in p_data.items():\n",
    "    m, m_forecast = run_prophet(value)\n",
    "    plot_prophet(m, m_forecast, site_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Results - Cross Validation\n",
    "All of these plots show that the overall trend of the predictions follows the observations, but there are still large variations between the prediction and the result. The plots are great, but I would like to see a statistical analysis of the error. Prophet has a mechanism for doing that using cross validation. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "def fbp_cross_validation(m, site_id):\n",
    "    df_cv = cross_validation(m, horizon='1 days', initial='35 days', period='1 days')\n",
    "    print(df_cv.head())\n",
    "\n",
    "df = p_data[466851]\n",
    "m, m_forecast = run_prophet(df)\n",
    "fbp_cross_validation(m, 466851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "610 days, 0:00:00\n"
    }
   ],
   "source": [
    "import datetime \n",
    "\n",
    "d1 = datetime.datetime(day=1, month=5, year = 2018)\n",
    "d2 = datetime.datetime(day=1, month=1, year=2020)\n",
    "\n",
    "print(d2 - d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}